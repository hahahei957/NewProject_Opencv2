{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征值提取\n",
    "from sklearn.feature_extraction import DictVectorizer   # 特征抽取\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "import numpy as np\n",
    "# 数据预处理\n",
    "from sklearn.preprocessing import MinMaxScaler         # 因为归一化属于对特征的预处理，所以在preprocessing包下面\n",
    "from sklearn.preprocessing import StandardScaler    # 标准化\n",
    "from sklearn.preprocessing import Imputer           # 缺失值\n",
    "# 对处理好的特征选择\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_transform处理的内容都要变成列表的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']\n",
      "[[0 1 1 1 0 1 1 0]\n",
      " [1 1 1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 实例化CountVectorizer\n",
    "vector = CountVectorizer()\n",
    "# 调用对象的fit_transform方法\n",
    "\n",
    "res = vector.fit_transform([\"life is short,i like python\",\"life is too long,i dislike python\"])\n",
    "\n",
    "print(res)                          # 特征中的数据值  其中由于单个字母没有啥实际含义，所以提取的特征值中不包含他们\n",
    "print(vector.get_feature_names())    # 得到每个特征的名字\n",
    "print(res.toarray())                 # 装换成矩阵类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city=上海', 'city=北京', 'city=深圳', 'temperature']\n",
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t100.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t60.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t30.0\n"
     ]
    }
   ],
   "source": [
    "# 字典中数据的特征值化\n",
    "def dictvec():\n",
    "    # 实例化\n",
    "    dic_res = DictVectorizer()\n",
    "    # 调用对象的fit_transform方法\n",
    "    # 传入数据为一个矩阵也就是np.array()类型\n",
    "    data = dic_res.fit_transform([{'city': '北京','temperature': 100}, {'city': '上海','temperature':60}, {'city': '深圳','temperature': 30}])\n",
    "    \n",
    "    print(dic_res.get_feature_names())\n",
    "    print(data)\n",
    "dictvec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 汉语文本通过jieba分词，在进行特征值化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con1:<generator object Tokenizer.cut at 0x0000025034099548>\n",
      "c1:今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。\n",
      "['不要', '今天', '后天', '大部分', '所以', '放弃', '明天', '晚上', '残酷', '每个', '绝对', '美好']\n",
      "ans:  (0, 1)\t2\n",
      "  (0, 8)\t2\n",
      "  (0, 6)\t2\n",
      "  (0, 2)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 5)\t1\n",
      "[[1 2 1 1 1 1 2 1 2 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 对汉语文本的数据进行特征值处理时，建议使用jieba进行处理，\n",
    "def cutword():\n",
    "    con1 = jieba.cut(\"今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\")\n",
    "    con2 = jieba.cut(\"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\")\n",
    "    con3 = jieba.cut(\"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\")\n",
    "    print(f'con1:{con1}')\n",
    "    # 转换成列表\n",
    "    content1 = list(con1)\n",
    "    # 拼接成字符串\n",
    "    c1 = ' '.join(content1)     # 因为fit_transform函数在识别中文的时候是以空格为分隔才能提取特征值\n",
    "    print(f\"c1:{c1}\")\n",
    "    return c1\n",
    "c1 = cutword()\n",
    "handle = CountVectorizer()\n",
    "# 传入数据为一个矩阵也就是np.array()类型\n",
    "ans = handle.fit_transform([c1])\n",
    "print(handle.get_feature_names())\n",
    "print(f\"ans:{ans}\")\n",
    "print(ans.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.          0.          0.          0.        ]\n",
      " [ 0.         10.         10.          8.33333333]\n",
      " [ 5.          5.          6.         10.        ]]\n"
     ]
    }
   ],
   "source": [
    "def normalize():                     # 将数据映射到feature_range里面设定的区间中\n",
    "    mm = MinMaxScaler(feature_range=(0, 10))\n",
    "    # 传入数据为一个矩阵也就是np.array()类型\n",
    "    data = mm.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,13,46]])\n",
    "    print(data)\n",
    "normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标准化处理，优于归一化处理，归一化处理收到异常值(异常最大最小值的)的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.06904497 -1.35873244  0.98058068]\n",
      " [-0.26726124  0.33968311  0.39223227]\n",
      " [ 1.33630621  1.01904933 -1.37281295]]\n"
     ]
    }
   ],
   "source": [
    "def stand():\n",
    "    # 标准化--缩放\n",
    "    std = StandardScaler()\n",
    "    # 传入数据为一个矩阵也就是np.array()类型\n",
    "    data = std.fit_transform([[ 1., -1., 3.],[ 2., 4., 2.],[ 4., 6., -1.]])    \n",
    "    print(data)\n",
    "\n",
    "stand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 缺失值处理(用numpy可以实现缺失值处理，但个人感觉sklearn这个处理更方便)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [4. 3.]\n",
      " [7. 6.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "def nan_processing():\n",
    "    im = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    data = im.fit_transform([[1, 2], [np.nan, 3], [7, 6]])\n",
    "    print(data)\n",
    "nan_processing() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用np实现缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. nan  7.]\n",
      "1 [False  True False]\n",
      "[2. 3. 6.]\n",
      "0 [False False False]\n",
      "[[1. 2.]\n",
      " [4. 3.]\n",
      " [7. 6.]]\n"
     ]
    }
   ],
   "source": [
    "def np_process_nan(data):\n",
    "    for i in range(data.shape[1]):\n",
    "        temp_col = data[:,i]\n",
    "        print(temp_col)\n",
    "        nan_num = np.count_nonzero(temp_col!=temp_col)\n",
    "        print(nan_num,temp_col!=temp_col)\n",
    "        if nan_num>0:\n",
    "            data_to_fill = np.mean(temp_col[temp_col==temp_col])\n",
    "            # 将均值填充掉那些空值\n",
    "            temp_col[temp_col!=temp_col]=data_to_fill\n",
    "            # 将更新的temp_col赋值给data上它所在的那一列\n",
    "            data[:,i] = temp_col\n",
    "    return data\n",
    "data = np.array([[1, 2], [np.nan, 3], [7, 6]])\n",
    "new_data = np_process_nan(data)\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n"
     ]
    }
   ],
   "source": [
    "data=\"adfsaafasdfasd\"\n",
    "print(data[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择-删除低方差的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [1 4]\n",
      " [1 1]]\n",
      "[False  True  True False]\n"
     ]
    }
   ],
   "source": [
    "def feature_select():\n",
    "    var = VarianceThreshold(threshold=0.2)    # 将方差低于0.2的那一列特征值剔除\n",
    "    data = var.fit_transform([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])\n",
    "    print(data)\n",
    "    # 查看保留下来了哪几列\n",
    "    saved = var.get_support()\n",
    "    print(saved)\n",
    "feature_select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
